{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "import numpy\n",
    "from __future__ import print_function\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the data\n",
    "data = pd.read_csv('hist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "    # fit scaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler = scaler.fit(train)\n",
    "    # transform train\n",
    "    train = train.reshape(train.shape[0], train.shape[1])\n",
    "    train_scaled = scaler.transform(train)\n",
    "    # transform test\n",
    "    test = test.reshape(test.shape[0], test.shape[1])\n",
    "    test_scaled = scaler.transform(test)\n",
    "\n",
    "    return scaler, train_scaled, test_scaled\n",
    "\n",
    "#Preparing the engine data for predicting RUL\n",
    "def prepare_engine(engine, scaler):\n",
    "    \n",
    "    #Removing unit number and time in cycles column\n",
    "    engine = engine.iloc[:,2:len(engine.columns)]\n",
    "    engine = engine.values\n",
    "    \n",
    "    #Scaling using trained scaler\n",
    "    engine = engine.reshape(engine.shape[0], engine.shape[1])\n",
    "    engine = scaler.transform(engine)\n",
    "\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning the same prediction variable RUL to each engine \n",
    "unit_2RUL = data.loc[:,['unit number', 'RUL']].groupby('unit number').count().to_dict()['RUL']\n",
    "data.loc[:,'RUL_correct'] = data.loc[:,'unit number']\n",
    "data.replace({\"RUL_correct\":unit_2RUL}, inplace = True)\n",
    "data.drop('RUL', axis = 1, inplace= True)\n",
    "data.rename(columns= {'RUL_correct':'RUL'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making number of rows in each test engine divisible by batch_size = 4\n",
    "def delete_extra_rows(df):\n",
    "    if (len(df['unit number'])%4 != 0):\n",
    "        return df.iloc[0:len(df)-(len(df)%4),:]\n",
    "    \n",
    "    else:\n",
    "        return df\n",
    "data = data.groupby('unit number').apply(delete_extra_rows).reset_index(drop= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting training and test data (last 20 engines)\n",
    "def get_test(df):\n",
    "    if(df['unit number'].values[0]>80):\n",
    "        return df\n",
    "\n",
    "def get_train(df):\n",
    "    if(df['unit number'].values[0]<=80):\n",
    "        return df\n",
    "        \n",
    "test = data.groupby('unit number').apply(get_test).reset_index(drop = True).dropna(axis= 0)\n",
    "train = data.groupby('unit number').apply(get_train).reset_index(drop = True).dropna(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing unit number and time in cycles column\n",
    "#For train\n",
    "train_series_x = train.iloc[:,2:len(data.columns)-1]\n",
    "train_series_y = train.iloc[:,-1]\n",
    "#For test\n",
    "test_series_x = test.iloc[:,2:len(data.columns)-1]\n",
    "test_series_y = test.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test_x = test_series_x.values\n",
    "raw_train_x = train_series_x.values\n",
    "\n",
    "raw_test_y = test_series_y.values\n",
    "raw_train_y = train_series_y.values\n",
    "\n",
    "# transform the scale of the data\n",
    "scaler, train_scaled, test_scaled = scale(raw_train_x, raw_test_x)\n",
    "\n",
    "#Renaming\n",
    "train_x = train_scaled\n",
    "test_x = test_scaled\n",
    "train_y = raw_train_y\n",
    "test_y = raw_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 73s 5ms/step - loss: 0.3853\n",
      "\n",
      "Epoch 2/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 74s 5ms/step - loss: 0.3798\n",
      "\n",
      "Epoch 3/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 68s 4ms/step - loss: 0.3798\n",
      "\n",
      "Epoch 4/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 68s 4ms/step - loss: 0.3797\n",
      "\n",
      "Epoch 5/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 68s 4ms/step - loss: 0.3797\n",
      "\n",
      "Epoch 6/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 69s 4ms/step - loss: 0.3798\n",
      "\n",
      "Epoch 7/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 69s 4ms/step - loss: 0.3797\n",
      "\n",
      "Epoch 8/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 68s 4ms/step - loss: 0.3797\n",
      "\n",
      "Epoch 9/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 69s 4ms/step - loss: 0.3797\n",
      "\n",
      "Epoch 10/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 69s 4ms/step - loss: 0.3797\n",
      "\n",
      "Epoch 11/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 69s 4ms/step - loss: 0.3797\n",
      "\n",
      "Epoch 12/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 69s 4ms/step - loss: 0.3791\n",
      "\n",
      "Epoch 13/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 69s 4ms/step - loss: 0.3787\n",
      "\n",
      "Epoch 14/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 68s 4ms/step - loss: 0.3787\n",
      "\n",
      "Epoch 15/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 69s 4ms/step - loss: 0.3787\n",
      "\n",
      "Epoch 16/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 68s 4ms/step - loss: 0.3787\n",
      "\n",
      "Epoch 17/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 70s 4ms/step - loss: 0.3787\n",
      "\n",
      "Epoch 18/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 69s 4ms/step - loss: 0.3787\n",
      "\n",
      "Epoch 19/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 69s 4ms/step - loss: 0.3787\n",
      "\n",
      "Epoch 20/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 68s 4ms/step - loss: 0.3787\n",
      "\n",
      "Epoch 21/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 69s 4ms/step - loss: 0.3787\n",
      "\n",
      "Epoch 22/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 68s 4ms/step - loss: 0.3787\n",
      "\n",
      "Epoch 23/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 69s 4ms/step - loss: 0.378716008 [============================>.] - ETA: 0s - loss: \n",
      "\n",
      "Epoch 24/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 69s 4ms/step - loss: 0.3787\n",
      "\n",
      "Epoch 25/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 69s 4ms/step - loss: 0.3787\n",
      "\n",
      "Epoch 26/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 69s 4ms/step - loss: 0.3787\n",
      "\n",
      "Epoch 27/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 70s 4ms/step - loss: 0.3787\n",
      "\n",
      "Epoch 28/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 69s 4ms/step - loss: 0.3787\n",
      "\n",
      "Epoch 29/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 69s 4ms/step - loss: 0.3786\n",
      "\n",
      "Epoch 30/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 71s 4ms/step - loss: 0.3787\n",
      "\n",
      "Epoch 31/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 69s 4ms/step - loss: 0.3786\n",
      "\n",
      "Epoch 32/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 70s 4ms/step - loss: 0.3786\n",
      "\n",
      "Epoch 33/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 70s 4ms/step - loss: 0.3786\n",
      "\n",
      "Epoch 34/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 71s 4ms/step - loss: 0.3786\n",
      "\n",
      "Epoch 35/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 70s 4ms/step - loss: 0.3786\n",
      "\n",
      "Epoch 36/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 70s 4ms/step - loss: 0.3786\n",
      "\n",
      "Epoch 37/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 69s 4ms/step - loss: 0.3785\n",
      "\n",
      "Epoch 38/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 71s 4ms/step - loss: 0.3785\n",
      "\n",
      "Epoch 39/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 70s 4ms/step - loss: 0.3785\n",
      "\n",
      "Epoch 40/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 70s 4ms/step - loss: 0.3785\n",
      "\n",
      "Epoch 41/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 71s 4ms/step - loss: 0.3785\n",
      "\n",
      "Epoch 42/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 71s 4ms/step - loss: 0.3785\n",
      "\n",
      "Epoch 43/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 71s 4ms/step - loss: 0.3785\n",
      "\n",
      "Epoch 44/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 72s 4ms/step - loss: 0.3785\n",
      "\n",
      "Epoch 45/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 71s 4ms/step - loss: 0.3785\n",
      "\n",
      "Epoch 46/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 72s 4ms/step - loss: 0.3785\n",
      "\n",
      "Epoch 47/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 72s 4ms/step - loss: 0.3785\n",
      "\n",
      "Epoch 48/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 72s 5ms/step - loss: 0.3785\n",
      "\n",
      "Epoch 49/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 71s 4ms/step - loss: 0.3785\n",
      "\n",
      "Epoch 50/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 72s 4ms/step - loss: 0.3785\n",
      "\n",
      "Epoch 51/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 72s 5ms/step - loss: 0.3785\n",
      "\n",
      "Epoch 52/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 72s 5ms/step - loss: 0.3785\n",
      "\n",
      "Epoch 53/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 72s 4ms/step - loss: 0.3785\n",
      "\n",
      "Epoch 54/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 73s 5ms/step - loss: 0.3785\n",
      "\n",
      "Epoch 55/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 73s 5ms/step - loss: 0.3784\n",
      "\n",
      "Epoch 56/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 73s 5ms/step - loss: 0.3784\n",
      "\n",
      "Epoch 57/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 73s 5ms/step - loss: 0.3785\n",
      "\n",
      "Epoch 58/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 80s 5ms/step - loss: 0.3784\n",
      "\n",
      "Epoch 59/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 82s 5ms/step - loss: 0.3783\n",
      "\n",
      "Epoch 60/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 95s 6ms/step - loss: 0.3782\n",
      "\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16008/16008 [==============================]16008/16008 [==============================] - 83s 5ms/step - loss: 0.3782\n",
      "\n",
      "Epoch 62/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 89s 6ms/step - loss: 0.3782\n",
      "\n",
      "Epoch 63/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 84s 5ms/step - loss: 0.3782\n",
      "\n",
      "Epoch 64/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 89s 6ms/step - loss: 0.3782\n",
      "\n",
      "Epoch 65/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 89s 6ms/step - loss: 0.3781\n",
      "\n",
      "Epoch 66/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 94s 6ms/step - loss: 0.3781\n",
      "\n",
      "Epoch 67/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 92s 6ms/step - loss: 0.378116008 [============================>.] - E\n",
      "\n",
      "Epoch 68/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 86s 5ms/step - loss: 0.3780\n",
      "\n",
      "Epoch 69/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 94s 6ms/step - loss: 0.3778\n",
      "\n",
      "Epoch 70/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 96s 6ms/step - loss: 0.3776\n",
      "\n",
      "Epoch 71/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 90s 6ms/step - loss: 0.3776\n",
      "\n",
      "Epoch 72/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 89s 6ms/step - loss: 0.3776\n",
      "\n",
      "Epoch 73/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 95s 6ms/step - loss: 0.3775\n",
      "\n",
      "Epoch 74/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 89s 6ms/step - loss: 0.3775\n",
      "\n",
      "Epoch 75/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 97s 6ms/step - loss: 0.37751600\n",
      "\n",
      "Epoch 76/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 94s 6ms/step - loss: 0.3775\n",
      "\n",
      "Epoch 77/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 100s 6ms/step - loss: 0.3775\n",
      "\n",
      "Epoch 78/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 102s 6ms/step - loss: 0.3775\n",
      "\n",
      "Epoch 79/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 107s 7ms/step - loss: 0.3775\n",
      "\n",
      "Epoch 80/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 91s 6ms/step - loss: 0.3774\n",
      "\n",
      "Epoch 81/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 94s 6ms/step - loss: 0.3774\n",
      "\n",
      "Epoch 82/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 86s 5ms/step - loss: 0.3775\n",
      "\n",
      "Epoch 83/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 85s 5ms/step - loss: 0.3775\n",
      "\n",
      "Epoch 84/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 79s 5ms/step - loss: 0.3774\n",
      "\n",
      "Epoch 85/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 81s 5ms/step - loss: 0.3774\n",
      "\n",
      "Epoch 86/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 84s 5ms/step - loss: 0.3774\n",
      "\n",
      "Epoch 87/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 83s 5ms/step - loss: 0.3774\n",
      "\n",
      "Epoch 88/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 105s 7ms/step - loss: 0.3774\n",
      "\n",
      "Epoch 89/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 98s 6ms/step - loss: 0.3775\n",
      "\n",
      "Epoch 90/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 96s 6ms/step - loss: 0.3774\n",
      "\n",
      "Epoch 91/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 97s 6ms/step - loss: 0.3775\n",
      "\n",
      "Epoch 92/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 96s 6ms/step - loss: 0.3775\n",
      "\n",
      "Epoch 93/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 100s 6ms/step - loss: 0.3775\n",
      "\n",
      "Epoch 94/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 96s 6ms/step - loss: 0.3774\n",
      "\n",
      "Epoch 95/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 98s 6ms/step - loss: 0.3776\n",
      "\n",
      "Epoch 96/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 92s 6ms/step - loss: 0.3776\n",
      "\n",
      "Epoch 97/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 96s 6ms/step - loss: 0.3775\n",
      "\n",
      "Epoch 98/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 97s 6ms/step - loss: 0.3775\n",
      "\n",
      "Epoch 99/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 93s 6ms/step - loss: 0.3775\n",
      "\n",
      "Epoch 100/100\n",
      "16008/16008 [==============================]16008/16008 [==============================] - 96s 6ms/step - loss: 0.3774\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timesteps = 1\n",
    "input_dim = train_x.shape[1]\n",
    "batch_size = 4\n",
    "size_latent = 6\n",
    "size_linear_layer = 5\n",
    "\n",
    "#Reshaping the input to be fit to be input to the LSTM\n",
    "train_x = train_x.reshape(np.shape(train_x)[0], timesteps, input_dim)\n",
    "\n",
    "inputs = Input(batch_shape=(batch_size,timesteps, input_dim))\n",
    "\n",
    "encoded = LSTM(size_latent, return_sequences = True)(inputs)\n",
    "encoded = LSTM(size_latent,return_sequences = True)(encoded)\n",
    "encoded = LSTM(size_latent,return_sequences = True)(encoded)\n",
    "encoded = Dense(size_linear_layer, activation = 'relu')(encoded)\n",
    "\n",
    "decoded = Dense(size_latent, activation = 'relu')(encoded)\n",
    "decoded =  LSTM(size_latent,return_sequences = True)(decoded)                                   \n",
    "decoded =  LSTM(size_latent,return_sequences = True)(decoded)\n",
    "decoded =  LSTM(size_latent, return_sequences = True)(decoded)\n",
    "decoded = Dense(input_dim, activation = 'relu')(decoded)\n",
    "\n",
    "sequence_autoencoder = tf.keras.models.Model(inputs, decoded)\n",
    "\n",
    "encoder = tf.keras.models.Model(inputs,encoded)\n",
    "\n",
    "sequence_autoencoder.compile(loss='mean_squared_error', optimizer='Adam')\n",
    "\n",
    "history_data = sequence_autoencoder.fit(train_x, train_x, epochs=100, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEWCAYAAABmE+CbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nP7qSyEygqLNiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2192344908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the training loss with epochs\n",
    "fig = plt.figure()\n",
    "plt.plot(history_data.history['loss'])\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For training the regression model for predicting RUL\n",
    "\n",
    "#Train data\n",
    "#Calculating the embeddings which would be input to the regression classifier\n",
    "#Calculating the output for each batch (RUL)\n",
    "\n",
    "embeddings = encoder.predict(train_x, batch_size= batch_size)\n",
    "start_batch = 0\n",
    "mean_embeddings_train = []\n",
    "output_train = []\n",
    "while(start_batch < train_x.shape[0] - 1):\n",
    "    mean_embeddings_train.append(list(np.mean(embeddings[start_batch: start_batch + batch_size, :], axis = 0)[0,:]))\n",
    "    output_train.append(train_y[start_batch: start_batch + batch_size][0])\n",
    "    start_batch += batch_size\n",
    "\n",
    "    \n",
    "#Test data\n",
    "#Reshaping test data to be of same shape to be input to the encoder\n",
    "test_x = test_x.reshape(np.shape(test_x)[0], timesteps, input_dim)\n",
    "\n",
    "embeddings = encoder.predict(test_x, batch_size= batch_size)\n",
    "start_batch = 0\n",
    "mean_embeddings_test = []\n",
    "output_test = []\n",
    "while(start_batch < test_x.shape[0] - 1):\n",
    "    mean_embeddings_test.append(list(np.mean(embeddings[start_batch: start_batch + batch_size, :], axis = 0)[0,:]))\n",
    "    output_test.append(test_y[start_batch: start_batch + batch_size][0])\n",
    "    start_batch += batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combing training and testing embeddings and outputs for grid search Cross validation\n",
    "mean_embeddings_train.extend(mean_embeddings_test)\n",
    "output_train.extend(output_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-530-c1ea0ee41b67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gamma'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_embeddings_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best Parameters: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf = SVR(kernel = 'rbf')\n",
    "parameters = {'C':[0.01, 0.1, 1, 10, 20, 50,100, 500], 'gamma': [0.01, 0.1, 1, 10, 50, 100]}\n",
    "gs = GridSearchCV(clf, parameters)\n",
    "gs.fit(mean_embeddings_train, output_train)\n",
    "best_params = gs.best_params_\n",
    "print(\"Best Parameters: {}\".format(gs.best_params_))\n",
    "print(\"Best test cross validation R2 score {}\".format(np.max(gs.cv_results_['mean_train_score'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=100, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.1,\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the regression model with best parameters\n",
    "clf = SVR(kernel = 'rbf', C = 100, gamma = 0.1)\n",
    "clf.fit(mean_embeddings_train, output_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engine_tul = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Engine: 101\n",
      "TULprediction at current cycle 5: 210.16065530261693\n",
      "TULprediction at current cycle 9: 209.2453343343088\n",
      "TULprediction at current cycle 13: 210.7392851004613\n",
      "TULprediction at current cycle 17: 210.894803674741\n",
      "TULprediction at current cycle 21: 211.098770712089\n",
      "TULprediction at current cycle 25: 210.70202008763792\n",
      "TULprediction at current cycle 29: 210.18683399095192\n",
      "\n",
      "\n",
      "Current Engine: 102\n",
      "TULprediction at current cycle 5: 203.41628892837238\n",
      "TULprediction at current cycle 9: 202.2459076064315\n",
      "TULprediction at current cycle 13: 201.56715851643352\n",
      "TULprediction at current cycle 17: 201.65778790580544\n",
      "TULprediction at current cycle 21: 201.6231004544291\n",
      "TULprediction at current cycle 25: 201.77695384284007\n",
      "TULprediction at current cycle 29: 201.46814032822408\n",
      "TULprediction at current cycle 33: 201.4395724097432\n",
      "TULprediction at current cycle 37: 201.46985839713307\n",
      "TULprediction at current cycle 41: 201.52583440364532\n",
      "TULprediction at current cycle 45: 201.419971525615\n",
      "\n",
      "\n",
      "Current Engine: 103\n",
      "TULprediction at current cycle 5: 195.8470108849828\n",
      "TULprediction at current cycle 9: 195.8470108849828\n",
      "TULprediction at current cycle 13: 195.84701088498278\n",
      "TULprediction at current cycle 17: 195.8470108849828\n",
      "TULprediction at current cycle 21: 195.8470108849828\n",
      "TULprediction at current cycle 25: 195.84701088498284\n",
      "TULprediction at current cycle 29: 195.84701088498284\n",
      "TULprediction at current cycle 33: 195.8470108849828\n",
      "TULprediction at current cycle 37: 195.8470108849828\n",
      "TULprediction at current cycle 41: 195.8470108849828\n",
      "TULprediction at current cycle 45: 195.84701088498284\n",
      "TULprediction at current cycle 49: 195.84701088498284\n",
      "TULprediction at current cycle 53: 195.84701088498284\n",
      "TULprediction at current cycle 57: 195.84701088498284\n",
      "TULprediction at current cycle 61: 195.84701088498284\n",
      "TULprediction at current cycle 65: 195.8470108849828\n",
      "TULprediction at current cycle 69: 195.8470108849828\n",
      "TULprediction at current cycle 73: 195.8470108849828\n",
      "TULprediction at current cycle 77: 195.8470108849828\n",
      "TULprediction at current cycle 81: 195.8470108849828\n",
      "TULprediction at current cycle 85: 195.8470108849828\n",
      "TULprediction at current cycle 89: 195.84701088498278\n",
      "TULprediction at current cycle 93: 195.84701088498278\n",
      "TULprediction at current cycle 97: 195.84701088498278\n",
      "TULprediction at current cycle 101: 195.84701088498278\n",
      "TULprediction at current cycle 105: 195.84701088498275\n",
      "TULprediction at current cycle 109: 195.84701088498275\n",
      "TULprediction at current cycle 113: 195.84701088498272\n",
      "TULprediction at current cycle 117: 195.84701088498272\n",
      "TULprediction at current cycle 121: 195.84701088498272\n",
      "TULprediction at current cycle 125: 195.8470108849827\n",
      "\n",
      "\n",
      "Current Engine: 104\n",
      "TULprediction at current cycle 5: 201.88946240977106\n",
      "TULprediction at current cycle 9: 201.55053021278607\n",
      "TULprediction at current cycle 13: 201.8436894609533\n",
      "TULprediction at current cycle 17: 201.73027501498734\n",
      "TULprediction at current cycle 21: 201.65153191671712\n",
      "TULprediction at current cycle 25: 201.69692664784156\n",
      "TULprediction at current cycle 29: 202.02059217405483\n",
      "TULprediction at current cycle 33: 201.73409156774497\n",
      "TULprediction at current cycle 37: 201.81937273402826\n",
      "TULprediction at current cycle 41: 201.77278095071603\n",
      "TULprediction at current cycle 45: 201.74128019471502\n",
      "TULprediction at current cycle 49: 201.71995572545117\n",
      "TULprediction at current cycle 53: 201.60290541443143\n",
      "TULprediction at current cycle 57: 201.4887940446125\n",
      "TULprediction at current cycle 61: 201.5048568946458\n",
      "TULprediction at current cycle 65: 201.46411728893847\n",
      "TULprediction at current cycle 69: 201.36387889351946\n",
      "TULprediction at current cycle 73: 201.20008632871452\n",
      "TULprediction at current cycle 77: 201.10820379808243\n",
      "TULprediction at current cycle 81: 201.05861206336746\n",
      "TULprediction at current cycle 85: 201.01527013002033\n",
      "TULprediction at current cycle 89: 200.986617200012\n",
      "TULprediction at current cycle 93: 200.94317279586875\n",
      "TULprediction at current cycle 97: 200.90193426445094\n",
      "TULprediction at current cycle 101: 200.85755576056178\n",
      "TULprediction at current cycle 105: 200.8095075378957\n",
      "\n",
      "\n",
      "Current Engine: 124\n",
      "TULprediction at current cycle 5: 221.92877939101297\n",
      "TULprediction at current cycle 9: 221.65608715587445\n",
      "TULprediction at current cycle 13: 221.21241590303873\n",
      "TULprediction at current cycle 17: 220.23852529747313\n",
      "TULprediction at current cycle 21: 222.51550183034777\n",
      "TULprediction at current cycle 25: 221.9033616180541\n",
      "TULprediction at current cycle 29: 221.81229334968546\n",
      "TULprediction at current cycle 33: 221.30417348470675\n",
      "TULprediction at current cycle 37: 221.12448917410424\n",
      "TULprediction at current cycle 41: 220.7312025323625\n",
      "TULprediction at current cycle 45: 221.05245492327322\n",
      "TULprediction at current cycle 49: 221.169257235093\n",
      "TULprediction at current cycle 53: 221.2226716350383\n",
      "TULprediction at current cycle 57: 220.93638230413808\n",
      "TULprediction at current cycle 61: 221.138404572576\n",
      "TULprediction at current cycle 65: 220.96698023611813\n",
      "TULprediction at current cycle 69: 220.89066464345143\n",
      "TULprediction at current cycle 73: 220.8204933195471\n",
      "TULprediction at current cycle 77: 220.61626768712168\n",
      "TULprediction at current cycle 81: 220.45114225413317\n",
      "TULprediction at current cycle 85: 220.29594532081268\n",
      "TULprediction at current cycle 89: 220.27153617378383\n",
      "TULprediction at current cycle 93: 219.9123839176617\n",
      "TULprediction at current cycle 97: 219.91602219106994\n",
      "TULprediction at current cycle 101: 219.43946248578683\n",
      "TULprediction at current cycle 105: 219.20430098406771\n",
      "TULprediction at current cycle 109: 218.7940246561807\n",
      "TULprediction at current cycle 113: 218.49591188586916\n",
      "TULprediction at current cycle 117: 218.15013534122033\n",
      "TULprediction at current cycle 121: 217.97000782765764\n",
      "TULprediction at current cycle 125: 217.67500648486418\n",
      "TULprediction at current cycle 129: 217.53809262809818\n",
      "TULprediction at current cycle 133: 217.2713295817507\n",
      "TULprediction at current cycle 137: 217.08577897121484\n",
      "TULprediction at current cycle 141: 216.81026727502666\n",
      "TULprediction at current cycle 145: 216.48592527799892\n",
      "TULprediction at current cycle 149: 216.14292393281104\n",
      "TULprediction at current cycle 153: 215.84043300051363\n",
      "TULprediction at current cycle 157: 215.64600554711694\n",
      "TULprediction at current cycle 161: 215.37073722319616\n",
      "TULprediction at current cycle 165: 215.030535732854\n",
      "TULprediction at current cycle 169: 214.66603644248312\n",
      "TULprediction at current cycle 173: 214.3245103348602\n",
      "TULprediction at current cycle 177: 213.9910798161466\n",
      "TULprediction at current cycle 181: 213.6802521082512\n",
      "TULprediction at current cycle 185: 213.38032279816002\n",
      "\n",
      "\n",
      "Current Engine: 125\n",
      "TULprediction at current cycle 5: 204.4915608341788\n",
      "TULprediction at current cycle 9: 204.77432539982746\n",
      "TULprediction at current cycle 13: 205.21772664601073\n",
      "TULprediction at current cycle 17: 207.104769438348\n",
      "TULprediction at current cycle 21: 207.78612216814662\n",
      "TULprediction at current cycle 25: 207.4019504491359\n",
      "TULprediction at current cycle 29: 207.51259303550816\n",
      "TULprediction at current cycle 33: 207.3971811988372\n",
      "TULprediction at current cycle 37: 207.5414898031587\n",
      "TULprediction at current cycle 41: 207.1478668054474\n",
      "TULprediction at current cycle 45: 207.20098329737965\n",
      "\n",
      "\n",
      "Current Engine: 134\n",
      "TULprediction at current cycle 5: 201.88946240977106\n",
      "TULprediction at current cycle 9: 201.55053021278607\n",
      "TULprediction at current cycle 13: 201.8436894609533\n",
      "TULprediction at current cycle 17: 201.73027501498734\n",
      "TULprediction at current cycle 21: 201.65153191671712\n",
      "TULprediction at current cycle 25: 201.69692664784156\n",
      "TULprediction at current cycle 29: 202.02059217405483\n",
      "TULprediction at current cycle 33: 201.73409156774497\n",
      "TULprediction at current cycle 37: 201.81937273402826\n",
      "TULprediction at current cycle 41: 201.77278095071603\n",
      "TULprediction at current cycle 45: 201.74128019471502\n",
      "TULprediction at current cycle 49: 201.71995572545117\n",
      "TULprediction at current cycle 53: 201.60290541443143\n",
      "TULprediction at current cycle 57: 201.4887940446125\n",
      "TULprediction at current cycle 61: 201.5048568946458\n",
      "TULprediction at current cycle 65: 201.46411728893847\n",
      "TULprediction at current cycle 69: 201.36387889351946\n",
      "TULprediction at current cycle 73: 201.20008632871452\n",
      "TULprediction at current cycle 77: 201.10820379808243\n",
      "TULprediction at current cycle 81: 201.05861206336746\n",
      "TULprediction at current cycle 85: 201.01527013002033\n",
      "TULprediction at current cycle 89: 200.986617200012\n",
      "TULprediction at current cycle 93: 200.94317279586875\n",
      "TULprediction at current cycle 97: 200.90193426445094\n",
      "TULprediction at current cycle 101: 200.85755576056178\n",
      "TULprediction at current cycle 105: 200.8095075378957\n",
      "TULprediction at current cycle 109: 201.06718753916513\n",
      "TULprediction at current cycle 113: 201.81230013113318\n",
      "TULprediction at current cycle 117: 202.65608016076104\n",
      "TULprediction at current cycle 121: 203.224952448057\n",
      "TULprediction at current cycle 125: 203.78321249629337\n",
      "TULprediction at current cycle 129: 204.2401213256207\n",
      "TULprediction at current cycle 133: 205.0292891448371\n",
      "TULprediction at current cycle 137: 205.61753201751938\n",
      "TULprediction at current cycle 141: 205.9556370956067\n",
      "TULprediction at current cycle 145: 206.4107184363167\n",
      "TULprediction at current cycle 149: 206.7504036769663\n",
      "TULprediction at current cycle 153: 207.04641294374503\n",
      "TULprediction at current cycle 157: 207.31801812492168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TULprediction at current cycle 161: 207.6915960039102\n",
      "TULprediction at current cycle 165: 208.08308067829904\n",
      "TULprediction at current cycle 169: 208.2093170552503\n",
      "TULprediction at current cycle 173: 208.39895588824618\n",
      "TULprediction at current cycle 177: 208.54189652105254\n",
      "TULprediction at current cycle 181: 208.76174618284904\n",
      "TULprediction at current cycle 185: 208.98209124225997\n",
      "TULprediction at current cycle 189: 209.10027815305284\n",
      "TULprediction at current cycle 193: 209.25030690569\n",
      "TULprediction at current cycle 197: 209.38735978991642\n",
      "TULprediction at current cycle 201: 209.4128665353209\n",
      "TULprediction at current cycle 205: 209.52385754495214\n",
      "TULprediction at current cycle 209: 209.82545372014027\n",
      "TULprediction at current cycle 213: 209.85360276900715\n",
      "TULprediction at current cycle 217: 209.8849111899754\n",
      "TULprediction at current cycle 221: 209.8471864232389\n",
      "TULprediction at current cycle 225: 209.77207238092265\n",
      "TULprediction at current cycle 229: 209.74670549861628\n",
      "TULprediction at current cycle 233: 209.72410754701826\n",
      "TULprediction at current cycle 237: 209.60592341689042\n",
      "TULprediction at current cycle 241: 209.5432762753547\n",
      "TULprediction at current cycle 245: 209.4979269890813\n",
      "TULprediction at current cycle 249: 209.40625140208212\n",
      "TULprediction at current cycle 253: 209.27242689128687\n",
      "TULprediction at current cycle 257: 209.15118155843885\n",
      "TULprediction at current cycle 261: 209.00709802835203\n",
      "TULprediction at current cycle 265: 208.86652606861892\n",
      "TULprediction at current cycle 269: 208.7376371039138\n",
      "TULprediction at current cycle 273: 208.5949529819361\n",
      "TULprediction at current cycle 277: 208.46043975212402\n",
      "TULprediction at current cycle 281: 208.32402077149158\n",
      "TULprediction at current cycle 285: 208.20938781143883\n",
      "TULprediction at current cycle 289: 208.09751898041884\n",
      "TULprediction at current cycle 293: 208.00989248527983\n",
      "TULprediction at current cycle 297: 207.93048590835292\n",
      "TULprediction at current cycle 301: 207.8533009494823\n"
     ]
    }
   ],
   "source": [
    "#Walk forward validation method for predicting the RUL for testing\n",
    "\n",
    "for engine_no in [101, 102, 103, 104, 124, 125, 134]:\n",
    "    \n",
    "    #Initializing the engine tul dictionary\n",
    "    engine_tul[str(engine_no)]= []\n",
    "    \n",
    "    \n",
    "    #Reading the engine file\n",
    "    file_name = 'Engine' + str(engine_no) + '.csv'\n",
    "    engine = pd.read_csv(file_name)\n",
    "    \n",
    "    if(engine_no == 103):\n",
    "        engine.loc[:,'unit number'] = 3\n",
    "        \n",
    "    #Preparing the engine for the prediction\n",
    "    engine = prepare_engine(engine, scaler)\n",
    "    \n",
    "    print('\\n')\n",
    "    print('Current Engine: {}'.format(engine_no))\n",
    "    \n",
    "    \n",
    "    start_batch = 0\n",
    "    predicted_rul = []\n",
    "    while(start_batch + batch_size < len(engine) -1):\n",
    "\n",
    "\n",
    "        #Get current batch from the full engine data\n",
    "        cur_engine = engine[start_batch: start_batch + batch_size,:]\n",
    "\n",
    "        #Reshaping the current engine batch to be input to the Autoencoder\n",
    "        cur_engine = cur_engine.reshape(cur_engine.shape[0], timesteps, cur_engine.shape[1])\n",
    "\n",
    "        #Calculating the embedding vector for the current batch\n",
    "        embeddings = encoder.predict(cur_engine, batch_size = batch_size)\n",
    "\n",
    "        #Taking the mean of the embeddings for this batch\n",
    "        mean_embeddings = np.mean(embeddings, axis = 0)\n",
    "\n",
    "        #Using regression model to predict the output RUL for current batch\n",
    "        cur_rul = clf.predict(mean_embeddings)\n",
    "\n",
    "        #Appending the cur_rul to the previous batch rul vector\n",
    "        predicted_rul.append(cur_rul)\n",
    "\n",
    "        #RUL prediction at the current timestamp\n",
    "        print(\"TULprediction at current cycle {}: {}\".format(start_batch + batch_size + 1, np.mean(predicted_rul)))\n",
    "        \n",
    "        engine_tul[str(engine_no)].append(np.mean(predicted_rul))\n",
    "        \n",
    "        #Updating the start_batch\n",
    "        start_batch+=batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUL for engine 101: 179\n",
      "RUL for engine 102: 152\n",
      "RUL for engine 103: 69\n",
      "RUL for engine 104: 95\n",
      "RUL for engine 124: 32\n",
      "RUL for engine 125: 158\n",
      "RUL for engine 134: -189\n"
     ]
    }
   ],
   "source": [
    "#RUL prediction\n",
    "#Negative RUL means 0 RUL\n",
    "\n",
    "print('RUL for engine 101: {}'.format(int(np.mean(engine_tul['101']) - 31)))\n",
    "print('RUL for engine 102: {}'.format(int(np.mean(engine_tul['102']) - 49)))\n",
    "print('RUL for engine 103: {}'.format(int(np.mean(engine_tul['103']) - 126)))\n",
    "print('RUL for engine 104: {}'.format(int(np.mean(engine_tul['104']) - 106)))\n",
    "print('RUL for engine 124: {}'.format(int(np.mean(engine_tul['124']) - 186)))\n",
    "print('RUL for engine 125: {}'.format(int(np.mean(engine_tul['125']) - 48)))\n",
    "print('RUL for engine 134: {}'.format(int(np.mean(engine_tul['134']) - 395)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
